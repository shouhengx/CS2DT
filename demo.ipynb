{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3628f23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.backends.cudnn as cudnn\n",
    "import scipy.io as sio\n",
    "from scipy.io import savemat\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from vit_pytorch import ViT\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1098f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchsummary import summary\n",
    "import geniter\n",
    "import record\n",
    "import Utils\n",
    "import gc\n",
    "\n",
    "from thop import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b052bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_DATASET = 'IN'  # UP,IN,SV, KSC\n",
    "PARAM_EPOCH = 100\n",
    "PARAM_ITER = 3\n",
    "PATCH_SIZE = 4\n",
    "PARAM_VAL = 0.95\n",
    "mode='DEN-1'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "cross_attn_depth=1\n",
    "ssf_enc_depth=1\n",
    "\n",
    "\n",
    "P_dim = 128\n",
    "P_dim_head=64\n",
    "P_mlp_dim = 64\n",
    "P_depth = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e099ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "419dc7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341]\n",
    "dataset = PARAM_DATASET \n",
    "Dataset = dataset.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ea5db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(Dataset, split=0.9):\n",
    "    data_path = './../data/'\n",
    "    if Dataset == 'IN':\n",
    "        mat_data = sio.loadmat(data_path + 'Indian_pines_corrected.mat')\n",
    "        mat_gt = sio.loadmat(data_path + 'Indian_pines_gt.mat')\n",
    "        data_hsi = mat_data['indian_pines_corrected']\n",
    "        gt_hsi = mat_gt['indian_pines_gt']\n",
    "        K = 200\n",
    "        TOTAL_SIZE = 10249\n",
    "        VALIDATION_SPLIT = split\n",
    "        TRAIN_SIZE = math.ceil(TOTAL_SIZE * VALIDATION_SPLIT)\n",
    "\n",
    "    if Dataset == 'UP':\n",
    "        uPavia = sio.loadmat(data_path + 'PaviaU.mat')\n",
    "        gt_uPavia = sio.loadmat(data_path + 'PaviaU_gt.mat')\n",
    "        data_hsi = uPavia['paviaU']\n",
    "        gt_hsi = gt_uPavia['paviaU_gt']\n",
    "        K = 103\n",
    "        TOTAL_SIZE = 42776\n",
    "        VALIDATION_SPLIT = split\n",
    "        TRAIN_SIZE = math.ceil(TOTAL_SIZE * VALIDATION_SPLIT)\n",
    "\n",
    "    if Dataset == 'SV':\n",
    "        SV = sio.loadmat(data_path + 'Salinas_corrected.mat')\n",
    "        gt_SV = sio.loadmat(data_path + 'Salinas_gt.mat')\n",
    "        data_hsi = SV['salinas_corrected']\n",
    "        gt_hsi = gt_SV['salinas_gt']\n",
    "        K = data_hsi.shape[2]\n",
    "        TOTAL_SIZE = 54129\n",
    "        VALIDATION_SPLIT = split\n",
    "        TRAIN_SIZE = math.ceil(TOTAL_SIZE * VALIDATION_SPLIT)\n",
    "\n",
    "    if Dataset == 'KSC':\n",
    "        KSV = sio.loadmat(data_path + 'KSC.mat')\n",
    "        gt_KSV = sio.loadmat(data_path + 'KSC_gt.mat')\n",
    "        data_hsi = KSV['KSC']\n",
    "        gt_hsi = gt_KSV['KSC_gt']\n",
    "        K = data_hsi.shape[2]\n",
    "        TOTAL_SIZE = 5211\n",
    "        VALIDATION_SPLIT = split\n",
    "        TRAIN_SIZE = math.ceil(TOTAL_SIZE * VALIDATION_SPLIT)\n",
    "\n",
    "        \n",
    "    if Dataset == 'BO':\n",
    "        BO = sio.loadmat(data_path + 'Botswana.mat')\n",
    "        gt_BO = sio.loadmat(data_path + 'Botswana_gt.mat')\n",
    "        data_hsi = BO['Botswana']\n",
    "        gt_hsi = gt_BO['Botswana_gt']\n",
    "        K = data_hsi.shape[2]\n",
    "        TOTAL_SIZE = 3248\n",
    "        VALIDATION_SPLIT = split\n",
    "        TRAIN_SIZE = math.ceil(TOTAL_SIZE * VALIDATION_SPLIT)\n",
    "\n",
    "\n",
    "    if  Dataset == 'UH':\n",
    "        data_hsi = sio.loadmat(data_path + 'houston.mat')['houston']\n",
    "        gt_hsi = sio.loadmat(data_path + 'houston_gt.mat')['houston_gt_tr']\n",
    "        gt_hsi += sio.loadmat(data_path + 'houston_gt.mat')['houston_gt_te']\n",
    "        K = data_hsi.shape[2]\n",
    "        TOTAL_SIZE = 15029\n",
    "        VALIDATION_SPLIT = split\n",
    "        TRAIN_SIZE = math.ceil(TOTAL_SIZE * VALIDATION_SPLIT)\n",
    "\n",
    "\n",
    "    shapeor = data_hsi.shape\n",
    "    data_hsi = data_hsi.reshape(-1, data_hsi.shape[-1])\n",
    "    data_hsi = PCA(n_components=K).fit_transform(data_hsi)\n",
    "    shapeor = np.array(shapeor)\n",
    "    shapeor[-1] = K\n",
    "    data_hsi = data_hsi.reshape(shapeor)\n",
    "\n",
    "    return data_hsi, gt_hsi, TOTAL_SIZE, TRAIN_SIZE, VALIDATION_SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e231d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hsi, gt_hsi, TOTAL_SIZE, TRAIN_SIZE, VALIDATION_SPLIT = load_dataset(\n",
    "    Dataset, PARAM_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9eb21ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_x, image_y, BAND = data_hsi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dad1d1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(207400, 103)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_hsi.reshape(\n",
    "    np.prod(data_hsi.shape[:2]), np.prod(data_hsi.shape[2:]))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99d8eb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 340)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_hsi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42811dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = gt_hsi.reshape(np.prod(gt_hsi.shape[:2]), )\n",
    "gt.shape\n",
    "CLASSES_NUM = max(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dde8570",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_LENGTH = PATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e4a161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 2 * PATCH_LENGTH + 1\n",
    "img_cols = 2 * PATCH_LENGTH + 1\n",
    "img_channels = data_hsi.shape[2]\n",
    "INPUT_DIMENSION = data_hsi.shape[2]\n",
    "ALL_SIZE = data_hsi.shape[0] * data_hsi.shape[1]\n",
    "VAL_SIZE = int(TRAIN_SIZE)\n",
    "TEST_SIZE = TOTAL_SIZE - TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcdc17fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocessing.scale(data)\n",
    "data_ = data.reshape(data_hsi.shape[0], data_hsi.shape[1], data_hsi.shape[2])\n",
    "whole_data = data_\n",
    "padded_data = np.lib.pad(\n",
    "    whole_data, ((PATCH_LENGTH, PATCH_LENGTH), (PATCH_LENGTH, PATCH_LENGTH),\n",
    "                 (0, 0)),\n",
    "    'constant',\n",
    "    constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18f119f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(proportion, ground_truth):\n",
    "    train = {}\n",
    "    test = {}\n",
    "    labels_loc = {}\n",
    "    m = max(ground_truth)\n",
    "    for i in range(m):\n",
    "        indexes = [\n",
    "            j for j, x in enumerate(ground_truth.ravel().tolist())\n",
    "            if x == i + 1\n",
    "        ]\n",
    "        np.random.shuffle(indexes)\n",
    "        labels_loc[i] = indexes\n",
    "        if proportion != 1:\n",
    "            nb_val = max(int((1 - proportion) * len(indexes)), 3)\n",
    "        else:\n",
    "            nb_val = 0\n",
    "        train[i] = indexes[:nb_val]\n",
    "        test[i] = indexes[nb_val:]\n",
    "    train_indexes = []\n",
    "    test_indexes = []\n",
    "    for i in range(m):\n",
    "        train_indexes += train[i]\n",
    "        test_indexes += test[i]\n",
    "    np.random.shuffle(train_indexes)\n",
    "    np.random.shuffle(test_indexes)\n",
    "    return train_indexes, test_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17663dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Selecting Small Pieces from the Original Cube Data-----\n",
      "Train size:  (2135, 15, 15, 103)\n"
     ]
    }
   ],
   "source": [
    "index_iter=0\n",
    "np.random.seed(seeds[index_iter])\n",
    "train_indices, test_indices = sampling(VALIDATION_SPLIT, gt)\n",
    "_, total_indices = sampling(1, gt)\n",
    "\n",
    "TRAIN_SIZE = len(train_indices)\n",
    "TEST_SIZE = TOTAL_SIZE - TRAIN_SIZE\n",
    "VAL_SIZE = int(TRAIN_SIZE)\n",
    "\n",
    "print('-----Selecting Small Pieces from the Original Cube Data-----')\n",
    "x_train,y_train, x_val,y_val, x_test,y_test, all_data, gt_all = geniter.generate_iter(\n",
    "        TRAIN_SIZE, train_indices, TEST_SIZE, test_indices, TOTAL_SIZE,\n",
    "        total_indices, VAL_SIZE, whole_data, PATCH_LENGTH, padded_data,\n",
    "        INPUT_DIMENSION, 64, gt)  #batchsize in 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "924cba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#band_patch=1\n",
    "band=x_train.shape[-1]\n",
    "patch=x_train.shape[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a6ed803",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    image_size = patch,\n",
    "    num_patches = band,\n",
    "    num_classes = CLASSES_NUM,\n",
    "    dim = P_dim,\n",
    "    dim_head=P_dim_head,\n",
    "    mlp_dim = P_mlp_dim,\n",
    "    depth = P_depth,\n",
    "    heads = 4,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    mode = mode,\n",
    "    cross_attn_depth = cross_attn_depth, \n",
    "    ssf_enc_depth = ssf_enc_depth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b2e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1,patch,patch,band)\n",
    "flops, params = profile(model, (dummy_input,))\n",
    "print('flops: ', flops, 'params: ', params)\n",
    "print('flops: %.2f M, params: %.2f M' % (flops / 1000000.0, params / 1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f124df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,\n",
    "          train_iter,\n",
    "          valida_iter,\n",
    "          loss,\n",
    "          optimizer,\n",
    "          device,\n",
    "          epochs,\n",
    "          early_stopping=True,\n",
    "          early_num=20):\n",
    "    loss_list = [100]\n",
    "    early_epoch = 0\n",
    "\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    start = time.time()\n",
    "    train_loss_list = []\n",
    "    valida_loss_list = []\n",
    "    train_acc_list = []\n",
    "    valida_acc_list = []\n",
    "    for epoch in range(epochs):\n",
    "        train_acc_sum, n = 0.0, 0\n",
    "        time_epoch = time.time()\n",
    "        lr_adjust = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=PARAM_EPOCH//10, gamma=0.9)\n",
    "        for X, y in train_iter:\n",
    "\n",
    "            batch_count, train_l_sum = 0, 0\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y.long())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        lr_adjust.step()\n",
    "        valida_acc, valida_loss = record.evaluate_accuracy(\n",
    "            valida_iter, net, loss, device)\n",
    "        loss_list.append(valida_loss)\n",
    "\n",
    "        train_loss_list.append(train_l_sum)  # / batch_count)\n",
    "        train_acc_list.append(train_acc_sum / n)\n",
    "        valida_loss_list.append(valida_loss)\n",
    "        valida_acc_list.append(valida_acc)\n",
    "\n",
    "        print(\n",
    "            'epoch %d, train loss %.6f, train acc %.3f, valida loss %.6f, valida acc %.3f, time %.1f sec'\n",
    "            % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n,\n",
    "               valida_loss, valida_acc, time.time() - time_epoch))\n",
    "\n",
    "        PATH = \"./net_DBA.pt\"\n",
    "\n",
    "        if early_stopping and loss_list[-2] < loss_list[-1]:\n",
    "            if early_epoch == 0:\n",
    "                torch.save(net.state_dict(), PATH)\n",
    "            early_epoch += 1\n",
    "            loss_list[-1] = loss_list[-2]\n",
    "            if early_epoch == early_num:\n",
    "                net.load_state_dict(torch.load(PATH))\n",
    "                break\n",
    "        else:\n",
    "            early_epoch = 0\n",
    "\n",
    "    print('epoch %d, loss %.4f, train acc %.3f, time %.1f sec'\n",
    "          % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n,\n",
    "             time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa0e9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99d02140",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER = PARAM_ITER\n",
    "KAPPA = []\n",
    "OA = []\n",
    "AA = []\n",
    "TRAINING_TIME = []\n",
    "TESTING_TIME = []\n",
    "ELEMENT_ACC = np.zeros((ITER, CLASSES_NUM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_hsi,data,data_\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a49a3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index_iter in range(ITER):\n",
    "    print('iter:', index_iter)\n",
    "    \n",
    "    np.random.seed(seeds[index_iter])\n",
    "    train_indices, test_indices = sampling(VALIDATION_SPLIT, gt)\n",
    "    _, total_indices = sampling(1, gt)\n",
    "\n",
    "    TRAIN_SIZE = len(train_indices)\n",
    "    TEST_SIZE = TOTAL_SIZE - TRAIN_SIZE\n",
    "    VAL_SIZE = int(TRAIN_SIZE)\n",
    "\n",
    "    print('-----Selecting Small Pieces from the Original Cube Data-----')\n",
    "    x_train,y_train, x_val,y_val, x_test,y_test, all_data, gt_all = geniter.generate_iter(\n",
    "            TRAIN_SIZE, train_indices, TEST_SIZE, test_indices, TOTAL_SIZE,\n",
    "            total_indices, VAL_SIZE, whole_data, PATCH_LENGTH, padded_data,\n",
    "            INPUT_DIMENSION, 64, gt) \n",
    "\n",
    "    del all_data,gt_all\n",
    "    gc.collect()\n",
    "    \n",
    "    band=x_train.shape[-1]\n",
    "    patch=x_train.shape[-2]\n",
    "    \n",
    "    \n",
    "    x_train=torch.from_numpy(x_train).type(torch.FloatTensor) \n",
    "    y_train=torch.from_numpy(y_train).type(torch.FloatTensor) \n",
    "    Label_train=Data.TensorDataset(x_train,y_train)\n",
    "    \n",
    "    del x_train,y_train\n",
    "    gc.collect()\n",
    "    \n",
    "    x_test=torch.from_numpy(x_test).type(torch.FloatTensor)\n",
    "    y_test=torch.from_numpy(y_test).type(torch.FloatTensor) \n",
    "    Label_test=Data.TensorDataset(x_test,y_test)\n",
    "    \n",
    "    del x_test,y_test\n",
    "    gc.collect()\n",
    "    \n",
    "    x_val=torch.from_numpy(x_val).type(torch.FloatTensor)\n",
    "    y_val=torch.from_numpy(y_val).type(torch.FloatTensor)\n",
    "    Label_val=Data.TensorDataset(x_val,y_val)\n",
    "    \n",
    "    del x_val,y_val\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    label_train_loader=Data.DataLoader(Label_train,batch_size=64,shuffle=True)\n",
    "    label_test_loader=Data.DataLoader(Label_test,batch_size=64,shuffle=False)\n",
    "    label_val_loader=Data.DataLoader(Label_val,batch_size=64,shuffle=False)\n",
    "    \n",
    "    del Label_train,Label_test,Label_val\n",
    "    gc.collect()\n",
    "    model = ViT(\n",
    "    image_size = patch,\n",
    "    num_patches = band,\n",
    "    num_classes = CLASSES_NUM,\n",
    "    dim = P_dim,\n",
    "    dim_head=P_dim_head,\n",
    "    mlp_dim = P_mlp_dim,\n",
    "    depth = P_depth,\n",
    "    heads = 4,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    mode = mode,\n",
    "    cross_attn_depth = cross_attn_depth, \n",
    "    ssf_enc_depth = ssf_enc_depth\n",
    ")\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=5e-4,\n",
    "        weight_decay=0)\n",
    "\n",
    "    \n",
    "    tic1 = time.time()\n",
    "    train(\n",
    "            model,\n",
    "            label_train_loader,\n",
    "            label_val_loader,\n",
    "            loss,\n",
    "            optimizer,\n",
    "            device,\n",
    "            epochs=PARAM_EPOCH)\n",
    "    toc1 = time.time()\n",
    "    \n",
    "    \n",
    "    \n",
    "    pred_test = []\n",
    "    tic2 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for X, y in label_test_loader:\n",
    "            X = X.to(device)\n",
    "            model.eval()\n",
    "            y_hat = model(X)\n",
    "            pred_test.extend(np.array(model(X).cpu().argmax(axis=1)))\n",
    "    toc2 = time.time()\n",
    "    collections.Counter(pred_test)\n",
    "    gt_test = gt[test_indices] - 1\n",
    "\n",
    "    overall_acc = metrics.accuracy_score(pred_test, gt_test[:-VAL_SIZE])\n",
    "    confusion_matrix = metrics.confusion_matrix(pred_test, gt_test[:-VAL_SIZE])\n",
    "    each_acc, average_acc = record.aa_and_each_accuracy(confusion_matrix)\n",
    "    kappa = metrics.cohen_kappa_score(pred_test, gt_test[:-VAL_SIZE])\n",
    "    \n",
    "    KAPPA.append(kappa)\n",
    "    OA.append(overall_acc)\n",
    "    AA.append(average_acc)\n",
    "    TRAINING_TIME.append(toc1 - tic1)\n",
    "    TESTING_TIME.append(toc2 - tic2)\n",
    "    ELEMENT_ACC[index_iter, :] = each_acc\n",
    "    \n",
    "    del label_train_loader,label_test_loader,label_val_loader\n",
    "    gc.collect()\n",
    "    \n",
    "print(\"--------\" + \" Training Finished-----------\")\n",
    "record.record_output(\n",
    "    OA, AA, KAPPA, ELEMENT_ACC, TRAINING_TIME, TESTING_TIME, flops, params,\n",
    "    './report/' + 'DC-DenseFormer_'+ Dataset + '_' +str(mode)+'_dep_'+str(P_depth)+'_SSF_'+str(ssf_enc_depth)+'_Cro_'+str(cross_attn_depth)+'_Patch_'+ str(img_rows) + '_' +'spl'\n",
    "    + str(VALIDATION_SPLIT) +'.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train, x_val,y_val, x_test,y_test, all_data, gt_all = geniter.generate_iter(\n",
    "            TRAIN_SIZE, train_indices, TEST_SIZE, test_indices, TOTAL_SIZE,\n",
    "            total_indices, VAL_SIZE, whole_data, PATCH_LENGTH, padded_data,\n",
    "            INPUT_DIMENSION, 16, gt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c309635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del whole_data,padded_data,x_train,y_train, x_val,y_val, x_test,y_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "695f5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all=torch.from_numpy(all_data).type(torch.FloatTensor)\n",
    "y_all=torch.from_numpy(gt_all).type(torch.FloatTensor)\n",
    "Label_all=Data.TensorDataset(x_all,y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5940ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_all,y_all\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91e662ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_loader=Data.DataLoader(Label_all,batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691525cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Utils.generate_png(\n",
    "    label_all_loader, model, gt_hsi, Dataset, device, total_indices,\n",
    "    './classification_maps/' + 'DC-DenseFormer_'+ Dataset + '_' +str(mode)+'_dep_'+str(P_depth)+'_SSF_'+str(ssf_enc_depth)+'_Cro_'+str(cross_attn_depth)+'_Patch_'+ str(img_rows) + '_' +'spl'\n",
    "    + str(VALIDATION_SPLIT))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyHSI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f0e2fe85115fd2386557c42e0d23c6fb95dd61fe2d1e647d69acf95e1a29035"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
